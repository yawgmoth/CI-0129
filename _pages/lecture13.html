---
permalink: /slides/lecture13.html
---

<!DOCTYPE html>
<html>
  <head>
    <title>Lecture 13: Neural Networks</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; font-size: 2em; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .MathJax {
         font-size: 0.7em !important;
      }
      p { font-size: 1.25em; }
      div { font-size: 1.25em; }
      li { font-size: 1.25em; }
      
      .tiny li {  font-size: 0.8em; }
      
      .tiny table {  font-size: 0.8em; }
      li p { line-height: 1.25em; font-size: 1.25em; }
      .red { color: #fa0000; }
      .large { font-size: 2em; }
      
      .tiny { font-size: 0.8em; }
      
      .small li {  font-size: 0.8em; }
      
      .medium li {  font-size: 1.1em; }
      
      
      .mmedium li {  font-size: 0.95em; }
      
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      
      .left-column {
        color: #777;
        width: 50%;
        height: 92%;
        float: left;
      }
        .left-column h2:last-of-type, .left-column h3:last-child {
          color: #000;
        }
      .right-column {
        width: 45%;
        float: right;
        padding-top: 1em;
        font-size: 1em;
      }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Artificial Intelligence

### Neural Networks

---

class: center, middle 

# Some Linear Algebra

---

class: small

# Functions 

Remember our function `\(f(\vec{x}) = y\)`

- This function takes a vector of real numbers and produces one real number

- Unlike vectors you may have seen before, this vector is really just "an ordered collection of numbers"

- For example: We want to predict the price of google stock given the day of the year, the temperature, the position of Mars in its orbit, and the number of Marvel movies released so far

- We construct a four-dimensional vector with one entry for each of these numbers 

- Our (supervised) learning algorithm then has to figure out how to turn these four values into a stock price (not all values may be relevant)

---

class: medium

# Vectors 

* Vectors are neat because we have mathematical operations defined on them: Addition, substraction, multiplication with a scalar, etc.

* One particularly important operation is the dot product:

$$
\vec{v} \cdot \vec{w} = \begin{pmatrix}v_1\\\\v_2\\\\\vdots\\\\v_n\end{pmatrix}\cdot\begin{pmatrix}w_1\\\\w_2\\\\\vdots\\\\w_n\end{pmatrix} =  v_1 \cdot w_1 + v_2 \cdot w_2 + \ldots + v_n \cdot w_n
$$

* We will use this to concisely define learning systems and algorithms!

---

class: medium

# Vector and Matrices and Tensors, oh my!

* Vector: An ordered list of numbers 

* Matrix: A grid of numbers 

* We could store a matrix in a vector, if we remember the dimensions!

* **Tensor**: What if we "need" more dimension?

* For example: We have 10000 images of 28x28 pixels. We store them in a 10000x28x28 tensor

* In memory the pixels will still be stored sequentially, the tensor is really just a different "view" on the data

---

# PyTorch 

* PyTorch is a python library for Machine Learning

* At every major Machine Learning conference last year, the majority of papers used PyTorch

* The other "big one" is Tensorflow, which is a bit older and has therefore more adoption in industry

* PyTorch is easier to get started with, and once you know the core concepts you can easily pick up Tensorflow, too

---

# PyTorch vs. Tensorflow 

<img src="/CI-2600/assets/img/pytorch_vs_tensorflow.png" width="100%"/>

---

class: medium

# PyTorch 

* (Almost) everything is a `pytorch.Tensor`!

* These tensors really are just views into a sequential array of numbers 

* Each tensor can also "remember" where it came from (e.g. if it is the result of an addition)

* This means, you can also view a tensor as a "tree" of computation, with the result at the top, and the inputs as the leaves

* You can also tell your tensors that the operations should be performed on the GPU

---

class: center, middle 

# Neural Networks

---

# Artificial Neural Networks


<p style="margin-top: 5cm; text-align: center; font-size: 2em">
What is an (Artificial) Neural Network?
</p>


---

# Neural Networks

What is a Neural Network?

 - A parameterized, non-linear function
 
 - ... that has a gradient (derivative)
 
What is a Neural Network **not**?

 - How the brain works 
 
 - A black box that no one understands
 
---

# Braaaaaains

<blockquote class="twitter-tweet" data-conversation="none" data-lang="es"><p lang="en" dir="ltr">4yo child is one thing but we need to explain how Megaphragma mymaripenne can fly and navigate with a brain of only 7400 neurons. Each neuron must be doing much more (1000x) than our Perceptron model explains. <a href="https://t.co/xNPq0RAgPj">pic.twitter.com/xNPq0RAgPj</a></p>&mdash; Mark Sugrue (@marksugruek) <a href="https://twitter.com/marksugruek/status/1206130412260646912?ref_src=twsrc%5Etfw">15 de diciembre de 2019</a></blockquote>

---

# A Linear Model

$$
y = \vec{w} \cdot \vec{x} + b
$$

Alternatively:

$$
y = \vec{w}' \cdot \vec{x}'
$$

Where we add b to the end of w and 1 to the end of x

---

# Activation Functions 

* Often we want to limit our output between 0 and 1 (or -1 and 1)

* We do this by sending the result of the linear combination through an activation function 

<img src="/CI-2600/assets/img/activationfunctions.png" width="100%"/>

---

# An Example: Boolean Operators

* Let's use an even simpler activation function: If the result of the linear combination is greater than 0, the output is 1, otherwise it is 0 (step function)

<img src="/CI-2600/assets/img/stepfunction.png" width="35%"/>

* How could we model a logical "or"? "and"? "nand"?

* How about an "xor"?

---

# Limitation

* Our simple model can not model XOR (technical reason: It is not linearly separable)

* What could we do?

* Maybe adding more Neurons helps?

---

# Artificial Neural Networks 

.left-column[
<img src="/CI-2600/assets/img/ANN.svg" width="100%"/>
]

.right-column[
We introduced a "hidden layer", which will hold intermediate values h

$$
\vec{h} = f_1(W_1 \cdot \vec{x})\\\\
y = f_2(\vec{w_2} \cdot \vec{h})\\\\
y = f_2(\vec{w_2} \cdot f_1(W_1 \cdot \vec{x}))
$$
]

---

# An Example 

<img src="/CI-2600/assets/img/xorann.png" width="60%"/>

What is the weight matrix for the hidden layer?

What is the weight vector for the output?

What happens for different inputs, e.g. 1 and 0?

---

# Weights

* How did we get these weights?

* What if the output is wrong? How do we update the weights?

* We need a measure of error

---

# Gradient Descent on Neural Networks 

* Take the (Sum/Mean) Squared Error!

Remember:
$$
y = f_2(\vec{w_2} \cdot f_1(W_1 \cdot \vec{x}))
$$

Error:

$$
(y - \hat{y})^2 = (f_2(\vec{w_2} \cdot f_1(W_1 \cdot \vec{x})) - \hat{y})^2 = \text{minimal}
$$

So we want:

$$
\nabla (y - \hat{y})^2 = \vec{0}
$$

---

# The Gradient

$$
\nabla (y - \hat{y})^2 = \vec{0} = \nabla (f_2(\vec{w_2} \cdot f_1(W_1 \cdot \vec{x})) - \hat{y})^2
$$

This means we need to calculate the gradient, with respect to the weights (i.e. what we want to change!)

$$
\frac{\text{d}}{\text{d} w_2} (y - \hat{y})^2 = \frac{\text{d}}{\text{d} w_2} (f_2(\vec{w_2} \cdot f_1(W_1 \cdot \vec{x})) - \hat{y})^2\\\\
\frac{\text{d}}{\text{d} W_1} (y - \hat{y})^2 = \frac{\text{d}}{\text{d} W_1} (f_2(\vec{w_2} \cdot f_1(W_1 \cdot \vec{x})) - \hat{y})^2
$$

---

# Takeaways

* For the weight matrix we can proceed the same way, with more chain rule applications

* If we have more layers, that means more chain rule applications

* Good news: PyTorch does all of that automatically (remember `requires_grad=True`?)

* So now we "have" the gradient

---

# Learning!

* Give the network an input sample 

* Record the output 

* Calculate the error **and the gradient**

* Change the weights "a little" in the opposite direction of the gradient

---

# Learning 

$$
y = f_2(\vec{w_2} \cdot f_1(W_1 \cdot \vec{x}))\\\\
\Delta w_2 = 2\cdot(y - \hat{y})\cdot \frac{\text{d}}{\text{d} w_2} f_2(\vec{w_2} \cdot \vec{h}) \cdot \vec{h} \\\\
w_2 = w_2 - \alpha\cdot \Delta w_2
$$

* How do we start?

* How often do we do update?

* What is `\(\alpha\)`?

* Any problems with this approach?

---

# Gradient Descent: Problems

<img src="/CI-2600/assets/img/localoptimum.png" width="60%"/>

---

# Gradient Descent: More Problems

<img src="/CI-2600/assets/img/learningproblems.png" width="100%"/>

---

# Gradient Descent 

* Gradient Descent is the basis for learning in neural networks

* We will see some ways to work around these problems 

* So far we have also only seen Neural Networks with a single numerical output (Regression)

* Next time: Neural Networks for classification

---
  
# References
  
  * [Machine Learning 4 All: Guides](https://ml4a.github.io/guides/)
  
  * [PyTorch](https://pytorch.org/)
  
  * [DeepLearning with PyTorch](https://pytorch.org/deep-learning-with-pytorch)
  
  * [Introduction to Neural Networks](http://mt-class.org/jhu/slides/lecture-nn-intro.pdf)
  

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-0.14.0.min.js">
    </script>
    <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript"></script>
    <script>
      var slideshow = remark.create();
      
       // Setup MathJax
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      });

      MathJax.Hub.Configured();
    </script>
  </body>
</html>
