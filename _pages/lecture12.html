---
permalink: /slides/lecture12.html
---

<!DOCTYPE html>
<html>
  <head>
    <title>Lecture 12: Machine Learning</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; font-size: 2em; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .MathJax {
         font-size: 0.7em !important;
      }
      p { font-size: 1.25em; }
      div { font-size: 1.25em; }
      li { font-size: 1.25em; }
      
      .tiny li {  font-size: 0.8em; }
      
      .tiny table {  font-size: 0.8em; }
      li p { line-height: 1.25em; font-size: 1.25em; }
      .red { color: #fa0000; }
      .large { font-size: 2em; }
      
      .tiny { font-size: 0.8em; }
      
      .small li {  font-size: 0.8em; }
      
      .medium li {  font-size: 1.1em; }
      
      
      .mmedium li {  font-size: 0.95em; }
      
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Artificial Intelligence

### Reinforcement Learning

---

class: medium

# The Problem 

  * We want an agent that "figures out" how to "perform well"
  
  * Given an environment in which the agent perform actions, we tell the agent which reward they get 
  
  * We generally assume that our states are discrete, and actions transition between them
  
  * The goal of the agent is to learn which actions are "good" to perform in which state 
  
  * Rewards may be negative, representing "punishment"

---

# The Problem 

<img src="/PF-3341/assets/img/rl.png" width="100%"/>

---

# Reinforcement Learning

  * The idea is that the agent performs several trial runs in the environment to determine a good policy
  
  * Compare to how a human player might learn how to play a game: Try some actions, observe the result 
  
  * Games are a natural fit for this kind of learning, but there are other applications as well!
  
---

# An Example 

<center>
<img src="/PF-3341/assets/img/qprob.png" width="80%"/>
</center>

---

# An Example 

<center>
<img src="/PF-3341/assets/img/qreward.png" width="80%"/>
</center>
  
---

class: medium

# Reinforcement Learning: Notation

  * `s` is a state 
  
  * `a` is an action
  
  * `T(s,a)` is the *state transition function*, it tells us which state we reach when we use action `a` in state `s`
  
  * `R(s)` is the (immediate) *reward* for a particular state 
  
  * `V(s)` is the (utility) value of a state

---

# Policies

  * `\(\pi(s)\)` is a *policy*
  
  * A policy tells the agent which action it should take in each state 
  
  * The goal of learning is to learn a "good" policy 
  
  * The *optimal policy* is usually written as `\(\pi^*(s)\)`
  
---

# A Policy 

<center>
<img src="/PF-3341/assets/img/qpolicy.png" width="80%"/>
</center>

---

# The Bellman Equation

  * `V(s)` is the (utility) value of a state
  
  * This utility value depends on the reward of the state, as well as all future rewards the agent will receive 
  
  * However, the future rewards depend on what the agent will do, i.e. what the policy says
  
$$
V^\pi(s) = R(s) + \gamma V(T(s, \pi(s)))
$$

For the optimal policy:

$$
V(s) = R(s) + \max_a \gamma V(T(s, a))
$$

---

# (Partial) Value Function

<center>
<img src="/PF-3341/assets/img/qvalue.png" width="50%"/><img src="/PF-3341/assets/img/qpolicy.png" width="50%"/>
</center>

---

# Learning 

  * The problem is: We don't generally know `V`, because we don't know `T(s,a)` (or `R(s)`) a priori
  
  * What can we do?
  
  * In our learning runs (episodes), we record the rewards we get, and use them to find an estimate of `V`
  
  * But we would also need to learn which state each action takes us to, in order to determine which action we should take 

---

# The Q function

  * Instead of learning `V` directly, we define a new function `Q(s,a)`, that satisfies `\(V(s) = \max_a Q(s,a)\)`

$$
Q(s,a) = R(s) + \max_{a'} \gamma Q(T(s,a),a')
$$

  * Now we learn the value of `Q` for "each" pair of state and action
  
  * The agent's policy is then `\(\pi(s) = \text{argmax}_a Q(s,a)\)`
  
  * How do we learn `Q`?
  
---

# Q-Learning 

  * We store Q as a table, with one row per state and one column per action
  
  * We then initialize the table "somehow"
  
  * During each training episode, we update the table when we are in a state s and perform the action a:

$$
Q(s,a) \leftarrow Q(s,a) + \alpha (R(s) + \gamma \max_{a'} Q(T(s,a), a') - Q(s,a))
$$

$$
Q(s,a) \leftarrow (1 - \alpha ) \cdot Q(s,a) + \alpha \cdot (R(s) + \gamma \max_{a'} Q(T(s,a), a'))
$$

---

# Q-Learning: Training

  * How do we train this agent?
  
  * We could just pick the action with the highest Q-value in our table 
  
  * But then the initial values of the table would guide the exploration
  
  * Instead, we use an exploration policy
  
  * This could be random, or `\(\varepsilon\)`-greedy
  
---

# Q-Table 

<img src="/PF-3341/assets/img/qtable.png" width="45%"/>

---

class: mmedium

# Markov Decision Processes 

  * So far, we have assumed actions will take us from one state to another deterministically
  
  * However, in many environments, transitions are non-deterministic 
  
  * Fortunately, we don't have to change much: Instead of a transition function `T(s,a)` we have transition probabilities `\(P(s' | s, a)\)`
  
  * Wherever we used `T(s,a)` we now use the expected value over all possible successor states
  
  * Note that with Q-Learning we did not have to learn `T(s,a)`, and we also do **not** have to learn `\(P(s' | s, a)\)` (model-free algorithm)
  
---

class: center, middle

# Some Applications

---

class: medium

# Applications 

  * Games are a natural fit for Reinforcement Learning 
  
  * Arguably the first ever AI agent was a "Reinforcement Learning" agent for checkers in 1959
  
  * The Backgammon agent TD-Gammon was another early success able to beat top players in 1992
  
  * Another popular application is robot control, such as for an inverted pendulum, autonomous cars, helicopters, etc. 

---

# Games

  * Games, particularly classic arcade/Atari/NES games, have a become a standard testing-environment for Reinforcement Learning
  
  * Often, the state is given to the agent in terms of the raw pixels of the screen
  
  * Reinforcement Learning agents have become *very* good at playing these games 
  
  * Sometimes they find exploits!
  
---

# Exploiting Pong

<iframe width="560" height="315" src="https://www.youtube.com/embed/naSpIBWqOjk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

---

# Inverted Pendulum 

<svg xmlns="http://www.w3.org/2000/svg" version="1.1"
  xmlns:xlink="http://www.w3.org/1999/xlink"
  width="600" height="400" viewBox="-150 70 300 330">
<title>Cart pendulum</title>
<defs>
	<marker id="EndArrow" viewBox="0 0 10 10" refX="10" refY="5"
		markerUnits="strokeWidth" orient="auto"
		markerWidth="10" markerHeight="10">
		<polyline points="10,5 0,10 4,5 0,0 10,5" fill="black" />
	</marker>
	<marker id="StartArrow" viewBox="0 0 10 10" refX="0" refY="5"
		markerUnits="strokeWidth" orient="auto"
		markerWidth="10" markerHeight="10">
		<polyline points="0,5 10,0 6,5 10,10 0,5" fill="black" />
	</marker>
</defs>
<circle cx="0" cy="80" r="20" fill="black" transform="rotate(-30,0,300)" />
<g style="stroke:black;stroke-width:1;fill:none">
	<line x1="-150" x2="150" y1="390" y2="390" />
	<line x1="0" x2="0" y1="300" y2="70" stroke-width=".3" />
	<g marker-end="url(#EndArrow)" >
		<line x1="-140" x2="-140" y1="280" y2="230" />
		<line x1="-140" x2="-90" y1="280" y2="280" />
		<line x1="80" x2="150" y1="327.5" y2="327.5" />
		<path d="M0,150 A150,150 0 0,0 -75,170" marker-start="url(#StartArrow)" />
	</g>
</g>
<g id="set2">
<g id="set1" style="stroke:black;stroke-width:1;">
	<line id="hash" x1="-145" x2="-150" y1="390" y2="400" />
	<use xlink:href="#hash" transform="translate(10)" />
	<use xlink:href="#hash" transform="translate(20)" />
	<use xlink:href="#hash" transform="translate(30)" />
	<use xlink:href="#hash" transform="translate(40)" />
	<use xlink:href="#hash" transform="translate(50)" />
</g>
<use xlink:href="#set1" transform="translate(60)" />
</g>
<use xlink:href="#set2" transform="translate(120)" />
<use xlink:href="#set1" transform="translate(240)" />
<g style="stroke:black;stroke-width:1;fill:rgb(170,170,170)">
	<circle id="wheel" cx="-50" cy="370" r="20" />
	<use xlink:href="#wheel" x="100" />
	<rect x="-80" y="305" width="160" height="45" />
	<path d="M-10,304 A11,19 0 0,1 10,304" />
	<rect x="-2.5" y="100" width="5" height="200" transform="rotate(-30,0,300)" />
</g>
<text font-size="15" font-style="italic" fill="black">
	<tspan x="-130" y="235">y</tspan>
	<tspan x="-95" y="295">x</tspan>
	<tspan x="-100" y="330">M</tspan>
	<tspan x="110" y="325">F</tspan><tspan x="110" dy="-10">&#x0279d;</tspan>
	<tspan x="-45" y="150">&#x03b8;</tspan>
	<tspan x="-70" y="225">l</tspan>
	<tspan x="-85" y="100">m</tspan>
</text>
</svg>

  
---

# Stanford Autonomous Helicopter 

<iframe width="560" height="315" src="https://www.youtube.com/embed/VCdxqn0fcnE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

---

# Circuit Design 

<img src="/PF-3341/assets/img/circuit.png" width="60%"/>

---

# References

  * Chapter 21 of *AI: A Modern Approach*, by Russel and Norvig

  * [A Painless Q-Learning Tutorial](http://mnemstudio.org/path-finding-q-learning-tutorial.htm)

  * [Faulty Reward Functions in the Wild](https://openai.com/blog/faulty-reward-functions/)
  
  * [Exploiting Q*Bert](https://www.theverge.com/tldr/2018/2/28/17062338/ai-agent-atari-q-bert-cracked-bug-cheat)  

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-0.14.0.min.js">
    </script>
    <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript"></script>
    <script>
      var slideshow = remark.create();
      
       // Setup MathJax
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      });

      MathJax.Hub.Configured();
    </script>
  </body>
</html>
